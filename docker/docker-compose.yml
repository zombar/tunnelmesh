# TESTING ENVIRONMENT ONLY - DO NOT USE IN PRODUCTION
#
# This docker-compose setup is designed for local development and testing of
# TunnelMesh's chunk-level replication features. It uses ephemeral storage
# (tmpfs) for both S3 data and SSH keys, meaning all data is lost on restart.
#
# SSH keys are regenerated on each container restart using deterministic key
# generation (--keygen-seed) based on the container hostname. This ensures
# each coordinator replica gets a unique, reproducible peer identity without
# needing persistent storage, which is ideal for testing replication scenarios.
#
# ⚠️  WARNING: Deterministic key generation REDUCES SECURITY and should NEVER
#             be used in production. For production deployments, use persistent
#             volumes and cryptographically secure random key generation.
#
# For production setups, see docs/DOCKER.md for proper configuration with
# persistent storage and secure key management.

services:
  coordinator:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    hostname: coordinator
    deploy:
      replicas: 2  # Default to 2 coordinators for replication testing
    entrypoint: ["/bin/bash", "/scripts/coordinator-entrypoint.sh"]
    volumes:
      - ./config/coordinator.yaml.template:/etc/tunnelmesh/coordinator.yaml.template:ro
      - ./scripts/coordinator-entrypoint.sh:/scripts/coordinator-entrypoint.sh:ro
      # Ephemeral storage (testing only) - all data lost on restart
      - type: tmpfs
        target: /var/lib/tunnelmesh
        tmpfs:
          size: 268435456  # 256MB
      - type: tmpfs
        target: /root/.tunnelmesh  # SSH keys regenerated deterministically
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker socket for container management
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    networks:
      - mesh-control
      # Remove static IP - let docker assign IPs dynamically for scaling
    environment:
      - TUNNELMESH_TOKEN=${TUNNELMESH_TOKEN}
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 3s

  client:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    deploy:
      replicas: 5
    entrypoint: ["/bin/bash", "/scripts/client-entrypoint.sh"]
    volumes:
      - ./config/peer.yaml.template:/etc/tunnelmesh/peer.yaml.template:ro
      - ./scripts/client-entrypoint.sh:/scripts/client-entrypoint.sh:ro
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    depends_on:
      coordinator:
        condition: service_started
    networks:
      - mesh-control
    environment:
      - SERVER_URL=http://coordinator:8080  # DNS round-robin across all coordinators
      - AUTH_TOKEN=${TUNNELMESH_TOKEN}
      - PING_INTERVAL=2
      - DISCOVERY_INTERVAL=20

  # Monitoring stack - shares coordinator's network namespace for mesh access
  # Note: These use network_mode: "service:coordinator" which binds to coordinator's container.
  # The restart: true ensures they restart when coordinator restarts/recreates.
  # Each coordinator replica runs its own monitoring stack.
  prometheus:
    image: prom/prometheus:latest
    container_name: tunnelmesh-prometheus
    # Share coordinator's network namespace to access mesh IPs via TUN
    network_mode: "service:coordinator"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.listen-address=:9090'
      - '--web.external-url=/prometheus/'
      - '--web.route-prefix=/prometheus/'
    volumes:
      - ../monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../monitoring/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - ../monitoring/targets:/targets:ro
      - prometheus-data:/prometheus
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/prometheus/-/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      coordinator:
        condition: service_healthy
        restart: true
    restart: unless-stopped

  sd-generator:
    build:
      context: ..
      dockerfile: monitoring/sd_generator/Dockerfile
    container_name: tunnelmesh-sd-generator
    # Share coordinator's network to reach coord API and resolve mesh DNS
    network_mode: "service:coordinator"
    environment:
      - COORD_SERVER_URL=http://localhost:8080
      - AUTH_TOKEN=${TUNNELMESH_TOKEN}
      - POLL_INTERVAL=30s
      - OUTPUT_FILE=/targets/peers.json
      - METRICS_PORT=9443
      - TLS_SKIP_VERIFY=true
    volumes:
      - ../monitoring/targets:/targets
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    depends_on:
      coordinator:
        condition: service_healthy
        restart: true
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: tunnelmesh-grafana
    # Share coordinator's network namespace
    network_mode: "service:coordinator"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=false
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_HTTP_PORT=3000
      - GF_SERVER_ROOT_URL=https://coordinator-node.tunnelmesh/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/var/lib/grafana/dashboards/tunnelmesh.json
    volumes:
      - ../monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ../monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s
    depends_on:
      coordinator:
        condition: service_healthy
        restart: true
      prometheus:
        condition: service_started
      loki:
        condition: service_started
    restart: unless-stopped

  loki:
    image: grafana/loki:latest
    container_name: tunnelmesh-loki
    # Share coordinator's network namespace to receive logs from mesh peers
    network_mode: "service:coordinator"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ../monitoring/loki/config.yaml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    healthcheck:
      # Use /metrics instead of /ready - single-instance Loki's readiness check can be flaky
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3100/metrics"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      coordinator:
        condition: service_healthy
        restart: true
    restart: unless-stopped

  benchmarker:
    build:
      context: ..
      dockerfile: docker/Dockerfile.benchmarker
    container_name: tunnelmesh-benchmarker
    # Share coordinator's network namespace to access mesh IPs via TUN
    network_mode: "service:coordinator"
    environment:
      - COORD_SERVER_URL=http://localhost:8080
      - AUTH_TOKEN=${TUNNELMESH_TOKEN}
      - LOCAL_PEER=benchmarker
      - BENCHMARK_INTERVAL=30s       # Start new batch every 30s
      - BENCHMARK_CONCURRENCY=3      # 3 concurrent transfers per batch
      - BENCHMARK_SIZE=100MB
      - OUTPUT_DIR=/results
      - TLS_SKIP_VERIFY=true
      # Randomized chaos testing (enabled by default)
      # Each transfer randomly picks from presets: clean, subtle, lossy-wifi,
      # mobile-3g, mobile-4g, satellite, congested, bandwidth-10/50/100mbps
      # - RANDOMIZE_CHAOS=false      # Disable to run all clean benchmarks
    volumes:
      - benchmark-results:/results
    depends_on:
      coordinator:
        condition: service_healthy
        restart: true
    restart: unless-stopped

networks:
  mesh-control:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/24

volumes:
  prometheus-data:
  grafana-data:
  loki-data:
  benchmark-results:
