# TESTING ENVIRONMENT ONLY - DO NOT USE IN PRODUCTION
#
# This docker-compose setup is designed for local development and testing of
# TunnelMesh's chunk-level replication features. It uses ephemeral storage
# (tmpfs) for both S3 data and SSH keys, meaning all data is lost on restart.
#
# Coordinator services:
# - coord-1: Primary coordinator exposed on port 8081 (localhost:8081)
# - coord-2: Secondary coordinator exposed on port 8082 (localhost:8082)
# - coord-3: Tertiary coordinator exposed on port 8083 (localhost:8083)
#
# For testing, coordinators use HTTP (not HTTPS) via TUNNELMESH_ALLOW_HTTP=true.
# This is INSECURE and should NEVER be used in production.
#
# All three coordinators start by default to test multi-coordinator replication.
# Coordinators discover each other automatically via the mesh and replicate
# S3 data in real-time using the chunk-level replication protocol.
#
# ⚠️  WARNING: Ephemeral storage (tmpfs) means all data is lost on restart.
#             For production deployments, use persistent volumes.

services:
  # Coordinator 1 - Primary (exposed to host)
  coord-1:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    hostname: coord-1
    container_name: tunnelmesh-coord-1
    entrypoint: ["/bin/bash", "/scripts/coordinator-entrypoint.sh"]
    volumes:
      - ./config/coordinator.yaml.template:/etc/tunnelmesh/coordinator.yaml.template:ro
      - ./scripts/coordinator-entrypoint.sh:/scripts/coordinator-entrypoint.sh:ro
      - type: tmpfs
        target: /var/lib/tunnelmesh
        tmpfs:
          size: 268435456  # 256MB
      - type: tmpfs
        target: /root/.tunnelmesh
      - /var/run/docker.sock:/var/run/docker.sock:ro
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    networks:
      - mesh-control
    ports:
      - "8081:8080"  # Exposed to host for joining from host machine
    environment:
      - TUNNELMESH_TOKEN=${TUNNELMESH_TOKEN}
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 3s

  # Coordinator 2 - Secondary (optional, for replication testing)
  coord-2:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    hostname: coord-2
    container_name: tunnelmesh-coord-2
    entrypoint: ["/bin/bash", "/scripts/coordinator-entrypoint.sh"]
    volumes:
      - ./config/coordinator.yaml.template:/etc/tunnelmesh/coordinator.yaml.template:ro
      - ./scripts/coordinator-entrypoint.sh:/scripts/coordinator-entrypoint.sh:ro
      - type: tmpfs
        target: /var/lib/tunnelmesh
        tmpfs:
          size: 268435456
      - type: tmpfs
        target: /root/.tunnelmesh
      - /var/run/docker.sock:/var/run/docker.sock:ro
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    networks:
      - mesh-control
    ports:
      - "8082:8080"  # Different port for coordinator 2
    environment:
      - TUNNELMESH_TOKEN=${TUNNELMESH_TOKEN}
    depends_on:
      coord-1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 3s

  # Coordinator 3 - Tertiary
  coord-3:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    hostname: coord-3
    container_name: tunnelmesh-coord-3
    entrypoint: ["/bin/bash", "/scripts/coordinator-entrypoint.sh"]
    volumes:
      - ./config/coordinator.yaml.template:/etc/tunnelmesh/coordinator.yaml.template:ro
      - ./scripts/coordinator-entrypoint.sh:/scripts/coordinator-entrypoint.sh:ro
      - type: tmpfs
        target: /var/lib/tunnelmesh
        tmpfs:
          size: 268435456
      - type: tmpfs
        target: /root/.tunnelmesh
      - /var/run/docker.sock:/var/run/docker.sock:ro
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    networks:
      - mesh-control
    ports:
      - "8083:8080"  # Different port for coordinator 3
    environment:
      - TUNNELMESH_TOKEN=${TUNNELMESH_TOKEN}
    depends_on:
      coord-1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 3s

  client:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    deploy:
      replicas: 5
    entrypoint: ["/bin/bash", "/scripts/client-entrypoint.sh"]
    volumes:
      - ./config/peer.yaml.template:/etc/tunnelmesh/peer.yaml.template:ro
      - ./scripts/client-entrypoint.sh:/scripts/client-entrypoint.sh:ro
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    depends_on:
      coord-1:
        condition: service_started
    networks:
      - mesh-control
    environment:
      - SERVER_URL=http://coord-1:8080
      - AUTH_TOKEN=${TUNNELMESH_TOKEN}
      - PING_INTERVAL=2
      - DISCOVERY_INTERVAL=20

  # Monitoring stack - shares coord-1's network namespace for mesh access
  # Note: These use network_mode: "service:coord-1" which binds to coord-1's container.
  # The restart: true ensures they restart when coord-1 restarts/recreates.
  # Monitoring stack is centralized on coord-1 but can scrape metrics from all coordinators.
  prometheus:
    image: prom/prometheus:latest
    container_name: tunnelmesh-prometheus
    # Share coord-1's network namespace to access mesh IPs via TUN
    network_mode: "service:coord-1"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.listen-address=:9090'
      - '--web.external-url=/prometheus/'
      - '--web.route-prefix=/prometheus/'
    volumes:
      - ../monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../monitoring/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - ../monitoring/targets:/targets:ro
      - prometheus-data:/prometheus
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/prometheus/-/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      coord-1:
        condition: service_healthy
        restart: true
    restart: unless-stopped

  sd-generator:
    build:
      context: ..
      dockerfile: monitoring/sd_generator/Dockerfile
    container_name: tunnelmesh-sd-generator
    # Share coord-1's network to reach coord API and resolve mesh DNS
    network_mode: "service:coord-1"
    environment:
      - COORD_SERVER_URL=http://localhost:8080
      - AUTH_TOKEN=${TUNNELMESH_TOKEN}
      - POLL_INTERVAL=30s
      - OUTPUT_FILE=/targets/peers.json
      - METRICS_PORT=9443
      - TLS_SKIP_VERIFY=true
    volumes:
      - ../monitoring/targets:/targets
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    depends_on:
      coord-1:
        condition: service_healthy
        restart: true
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: tunnelmesh-grafana
    # Share coord-1's network namespace
    network_mode: "service:coord-1"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=false
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_HTTP_PORT=3000
      - GF_SERVER_ROOT_URL=https://coordinator-node.tunnelmesh/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/var/lib/grafana/dashboards/tunnelmesh.json
    volumes:
      - ../monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ../monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s
    depends_on:
      coord-1:
        condition: service_healthy
        restart: true
      prometheus:
        condition: service_started
      loki:
        condition: service_started
    restart: unless-stopped

  loki:
    image: grafana/loki:latest
    container_name: tunnelmesh-loki
    # Share coord-1's network namespace to receive logs from mesh peers
    network_mode: "service:coord-1"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ../monitoring/loki/config.yaml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    healthcheck:
      # Use /metrics instead of /ready - single-instance Loki's readiness check can be flaky
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3100/metrics"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      coord-1:
        condition: service_healthy
        restart: true
    restart: unless-stopped

  benchmarker:
    build:
      context: ..
      dockerfile: docker/Dockerfile.benchmarker
    container_name: tunnelmesh-benchmarker
    # Share coord-1's network namespace to access mesh IPs via TUN
    network_mode: "service:coord-1"
    environment:
      - COORD_SERVER_URL=http://localhost:8080
      - AUTH_TOKEN=${TUNNELMESH_TOKEN}
      - LOCAL_PEER=benchmarker
      - BENCHMARK_INTERVAL=30s       # Start new batch every 30s
      - BENCHMARK_CONCURRENCY=3      # 3 concurrent transfers per batch
      - BENCHMARK_SIZE=100MB
      - OUTPUT_DIR=/results
      - TLS_SKIP_VERIFY=true
      # Randomized chaos testing (enabled by default)
      # Each transfer randomly picks from presets: clean, subtle, lossy-wifi,
      # mobile-3g, mobile-4g, satellite, congested, bandwidth-10/50/100mbps
      # - RANDOMIZE_CHAOS=false      # Disable to run all clean benchmarks
    volumes:
      - benchmark-results:/results
    depends_on:
      coord-1:
        condition: service_healthy
        restart: true
    restart: unless-stopped

networks:
  mesh-control:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/24

volumes:
  prometheus-data:
  grafana-data:
  loki-data:
  benchmark-results:
