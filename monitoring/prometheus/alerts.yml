groups:
  # Warning-level alerts (early indicators, non-urgent)
  - name: warnings
    rules:
      # Mesh: Elevated packet drops
      - alert: ElevatedPacketDrops
        expr: sum(rate(tunnelmesh_dropped_no_route_total[5m])) > 1 or sum(rate(tunnelmesh_dropped_no_tunnel_total[5m])) > 1
        for: 2m
        labels:
          severity: warning
          category: mesh
        annotations:
          summary: "Elevated packet drops detected"
          description: "Packet drop rate exceeds 1/s for 2 minutes"

      # Mesh: Reconnection attempts increasing
      - alert: ReconnectionAttempts
        expr: sum(rate(tunnelmesh_reconnects_total[5m])) > 0.1
        for: 3m
        labels:
          severity: warning
          category: mesh
        annotations:
          summary: "Frequent reconnection attempts"
          description: "Reconnection rate indicates unstable connections"

      # Mesh: Some peers disconnected
      - alert: PeerDisconnected
        expr: count(tunnelmesh_connection_state == 0) > 0
        for: 5m
        labels:
          severity: warning
          category: mesh
        annotations:
          summary: "Peer(s) disconnected"
          description: "One or more peers have been disconnected for 5+ minutes"

      # Mesh: Tunnel health degraded
      - alert: UnhealthyTunnels
        expr: (sum(tunnelmesh_active_tunnels) - sum(tunnelmesh_healthy_tunnels)) > 2
        for: 3m
        labels:
          severity: warning
          category: mesh
        annotations:
          summary: "Unhealthy tunnels detected"
          description: "Multiple tunnels are active but not healthy"

      # Mesh: Forwarder errors occurring
      - alert: ForwarderErrors
        expr: sum(rate(tunnelmesh_forwarder_errors_total[5m])) > 0.1
        for: 2m
        labels:
          severity: warning
          category: mesh
        annotations:
          summary: "Forwarder errors detected"
          description: "Forwarder error rate is elevated"

      # Mesh: Elevated packet filter drops (per source peer)
      - alert: ElevatedFilteredDrops
        expr: sum by (peer, source_peer) (rate(tunnelmesh_dropped_filtered_total[5m])) > 10
        for: 2m
        labels:
          severity: warning
          category: mesh
        annotations:
          summary: "High packet filter drop rate on {{ $labels.peer }}"
          description: "Packet filter blocking >10 packets/s from source '{{ $labels.source_peer }}' on {{ $labels.peer }} - possible attack or misconfiguration"

      # Mesh: Multiple source peers being blocked
      - alert: MultipleSourcesBlocked
        expr: count by (peer) (rate(tunnelmesh_dropped_filtered_total[5m]) > 1) > 3
        for: 5m
        labels:
          severity: warning
          category: mesh
        annotations:
          summary: "Multiple source peers being blocked on {{ $labels.peer }}"
          description: "Packets from more than 3 source peers are being blocked - review filter rules"

      # Mesh: No filter rules with default-deny mode
      - alert: NoFilterRules
        expr: tunnelmesh_filter_default_deny == 1 and sum by (peer) (tunnelmesh_filter_rules_total) == 0
        for: 10m
        labels:
          severity: warning
          category: mesh
        annotations:
          summary: "Default-deny filter with no allow rules on {{ $labels.peer }}"
          description: "Packet filter is in default-deny mode but has no allow rules - all traffic is being blocked"

      # Data: S3 quota approaching limit
      # Threshold: 75% provides 25% buffer for users to react before critical level
      - alert: S3QuotaWarning
        expr: tunnelmesh_s3_quota_used_percent > 75
        for: 5m
        labels:
          severity: warning
          category: data
        annotations:
          summary: "S3 storage quota approaching limit"
          description: "S3 storage is {{ $value }}% full (warning at 75%)"

      # Data: Elevated S3 error rate
      # Threshold: 0.1/s (6/min) indicates intermittent issues; 3min duration filters transient spikes
      - alert: S3ElevatedErrors
        expr: sum(rate(tunnelmesh_s3_requests_total{status=~"error|quota_exceeded"}[5m])) > 0.1
        for: 3m
        labels:
          severity: warning
          category: data
        annotations:
          summary: "Elevated S3 error rate"
          description: "S3 error rate exceeds 0.1/s for 3 minutes"

      # TODO: Uncomment when auth metrics are implemented
      # # Security: Elevated auth failures
      # - alert: ElevatedAuthFailures
      #   expr: sum(rate(tunnelmesh_auth_attempts_total{result!="success"}[5m])) > 0.5
      #   for: 3m
      #   labels:
      #     severity: warning
      #     category: security
      #   annotations:
      #     summary: "Elevated authentication failures"
      #     description: "Auth failure rate exceeds 0.5/s - possible credential issues or attack"

      # TODO: Uncomment when auth metrics are implemented
      # # Security: Elevated access denials
      # - alert: ElevatedAccessDenials
      #   expr: sum(rate(tunnelmesh_authz_checks_total{result="denied"}[5m])) > 1
      #   for: 5m
      #   labels:
      #     severity: warning
      #     category: security
      #   annotations:
      #     summary: "Elevated authorization denials"
      #     description: "Users frequently accessing resources they don't have permissions for"

      # TODO: Uncomment when auth metrics are implemented
      # # Security: User repeatedly denied
      # - alert: UserAccessDenied
      #   expr: sum(rate(tunnelmesh_authz_denied_total[5m])) by (user, bucket) > 0.1
      #   for: 5m
      #   labels:
      #     severity: warning
      #     category: security
      #   annotations:
      #     summary: "User repeatedly denied access"
      #     description: "User {{ $labels.user }} denied access to bucket {{ $labels.bucket }} - check permissions"

  # Critical-level alerts (service degradation, requires intervention)
  - name: critical
    rules:
      # Mesh: Multiple peers disconnected
      - alert: MultiplePeersDown
        expr: count(tunnelmesh_connection_state == 0) >= 3
        for: 3m
        labels:
          severity: critical
          category: mesh
        annotations:
          summary: "Multiple peers disconnected"
          description: "3+ peers are currently disconnected for 3+ minutes - mesh connectivity severely impacted"

      # Mesh: High error rate
      - alert: HighErrorRate
        expr: sum(rate(tunnelmesh_forwarder_errors_total[1m])) > 1
        for: 1m
        labels:
          severity: critical
          category: mesh
        annotations:
          summary: "High forwarder error rate"
          description: "Forwarder errors exceeding 1/s - service is degraded"

      # Mesh: Significant packet loss
      - alert: SignificantPacketLoss
        expr: (sum(rate(tunnelmesh_dropped_no_route_total[1m])) + sum(rate(tunnelmesh_dropped_no_tunnel_total[1m]))) / (sum(rate(tunnelmesh_packets_sent_total[1m])) + 1) > 0.05
        for: 2m
        labels:
          severity: critical
          category: mesh
        annotations:
          summary: "Significant packet loss detected"
          description: "Packet drop rate exceeds 5% of total traffic"

      # Mesh: No healthy tunnels for a peer
      - alert: NoHealthyTunnels
        expr: tunnelmesh_active_tunnels > 0 and tunnelmesh_healthy_tunnels == 0
        for: 2m
        labels:
          severity: critical
          category: mesh
        annotations:
          summary: "Peer has no healthy tunnels"
          description: "Active tunnels exist but none are healthy - connectivity impaired"

      # Mesh: Relay connection lost (affects NAT traversal)
      - alert: RelayDisconnected
        expr: tunnelmesh_relay_connected == 0
        for: 3m
        labels:
          severity: critical
          category: mesh
        annotations:
          summary: "Relay connection lost"
          description: "Peer lost connection to relay server for 3+ minutes - NAT traversal may fail"

      # Data: S3 quota critical level
      # Threshold: 85% leaves 15% buffer; action required to prevent quota exhaustion
      - alert: S3QuotaCritical
        expr: tunnelmesh_s3_quota_used_percent > 85
        for: 2m
        labels:
          severity: critical
          category: data
        annotations:
          summary: "S3 storage quota critical"
          description: "S3 storage is {{ $value }}% full (critical at 85%)"

      # Data: High S3 error rate
      # Threshold: >10% error rate indicates service degradation; 2min duration confirms persistent issue
      - alert: S3HighErrorRate
        expr: sum(rate(tunnelmesh_s3_requests_total{status=~"error"}[5m])) / sum(rate(tunnelmesh_s3_requests_total[5m])) > 0.1
        for: 2m
        labels:
          severity: critical
          category: data
        annotations:
          summary: "High S3 error rate"
          description: "{{ $value | humanizePercentage }} of S3 requests are failing"

      # Security: S3 access denied spike
      - alert: S3AccessDeniedSpike
        expr: sum(rate(tunnelmesh_s3_requests_total{status="access_denied"}[5m])) > 5
        for: 2m
        labels:
          severity: critical
          category: security
        annotations:
          summary: "Spike in S3 access denied responses"
          description: "Access denied rate exceeds 5/s - possible permissions misconfiguration or attack"

      # TODO: Uncomment when auth metrics are implemented
      # # Security: High auth failure rate
      # - alert: HighAuthFailureRate
      #   expr: sum(rate(tunnelmesh_auth_attempts_total{result!="success"}[5m])) / sum(rate(tunnelmesh_auth_attempts_total[5m])) > 0.5
      #   for: 2m
      #   labels:
      #     severity: critical
      #     category: security
      #   annotations:
      #     summary: "High authentication failure rate"
      #     description: "{{ $value | humanizePercentage }} of auth attempts failing - possible attack or config issue"

  # Page-level alerts (immediate action required, service outage)
  - name: page
    rules:
      # Mesh: All tunnels down
      - alert: AllTunnelsDown
        expr: sum(tunnelmesh_active_tunnels) == 0 and count(tunnelmesh_peer_info) > 1
        for: 30s
        labels:
          severity: page
          category: mesh
        annotations:
          summary: "All mesh tunnels are down"
          description: "Complete mesh network failure - no active tunnels between any peers"

      # Mesh: Majority of peers unreachable
      - alert: MajorOutage
        expr: count(tunnelmesh_connection_state == 0) / count(tunnelmesh_connection_state) > 0.5
        for: 1m
        labels:
          severity: page
          category: mesh
        annotations:
          summary: "Majority of peers unreachable"
          description: "Over 50% of peers are disconnected - major outage"

      # Mesh: Complete relay failure (when relay is configured)
      - alert: RelayCompleteFailure
        expr: count(tunnelmesh_relay_connected == 0) == count(tunnelmesh_relay_connected) and count(tunnelmesh_relay_connected) > 0
        for: 1m
        labels:
          severity: page
          category: mesh
        annotations:
          summary: "All peers lost relay connection"
          description: "Complete relay server failure - NAT traversal unavailable for all peers"

      # Mesh: No peers responding (scrape failure or complete outage)
      - alert: NoPeersResponding
        expr: absent(tunnelmesh_peer_info)
        for: 2m
        labels:
          severity: page
          category: mesh
        annotations:
          summary: "No TunnelMesh peers responding"
          description: "Cannot reach any TunnelMesh peer metrics - possible complete network outage"

      # Mesh: WireGuard concentrator down (if enabled)
      - alert: WireGuardDown
        expr: tunnelmesh_wireguard_enabled == 1 and tunnelmesh_wireguard_device_running == 0
        for: 1m
        labels:
          severity: page
          category: mesh
        annotations:
          summary: "WireGuard concentrator device down"
          description: "WireGuard is enabled but the device is not running"

      # Data: S3 service down
      # Note: Also triggers when idle; consider changing to `absent()` only if false positives occur
      - alert: S3Down
        expr: absent(tunnelmesh_s3_requests_total) or sum(rate(tunnelmesh_s3_requests_total[1m])) == 0
        for: 2m
        labels:
          severity: page
          category: data
        annotations:
          summary: "S3 service down or not receiving requests"
          description: "No S3 metrics reported for 2 minutes"

      # Data: S3 quota near full - immediate action required
      # Threshold: 92% leaves only 8% buffer; under high load this could fill in minutes
      - alert: S3QuotaNearFull
        expr: tunnelmesh_s3_quota_used_percent > 92
        for: 1m
        labels:
          severity: page
          category: data
        annotations:
          summary: "S3 storage quota near full - immediate action required"
          description: "S3 storage is {{ $value }}% full (>92%) - uploads will fail soon"

      # TODO: Uncomment when auth metrics are implemented
      # # Security: Possible brute force attack
      # - alert: PossibleBruteForce
      #   expr: sum(rate(tunnelmesh_auth_attempts_total{result!="success"}[1m])) > 10
      #   for: 30s
      #   labels:
      #     severity: page
      #     category: security
      #   annotations:
      #     summary: "Possible brute force attack"
      #     description: "Auth failure rate exceeds 10/s - possible credential brute force attack"

  # Stats Replication Monitoring
  - name: stats_replication
    rules:
      # Docker stats collection stalled
      - alert: DockerStatsCollectionFailed
        expr: time() - tunnelmesh_docker_stats_last_collection_timestamp > 300
        for: 2m
        labels:
          severity: warning
          category: data
        annotations:
          summary: "Docker stats collection stalled on {{ $labels.peer }}"
          description: "No Docker stats collected for 5+ minutes"

      # Docker stats collection errors
      - alert: DockerStatsCollectionErrors
        expr: rate(tunnelmesh_docker_stats_collection_errors_total[5m]) > 0.1
        for: 3m
        labels:
          severity: warning
          category: data
        annotations:
          summary: "Docker stats collection errors on {{ $labels.peer }}"
          description: "Collection failing with {{ $labels.error_type }} errors"

      # Docker stats persistence failure
      - alert: DockerStatsPersistenceFailed
        expr: rate(tunnelmesh_docker_stats_persistence_errors_total[5m]) > 0.1
        for: 3m
        labels:
          severity: warning
          category: data
        annotations:
          summary: "Docker stats S3 persistence failing"
          description: "Cannot save Docker stats on {{ $labels.peer }}"

      # Network stats save stalled
      - alert: NetworkStatsSaveFailed
        expr: time() - tunnelmesh_network_stats_last_save_timestamp > 600
        for: 2m
        labels:
          severity: warning
          category: data
        annotations:
          summary: "Network stats save stalled"
          description: "Stats for {{ $labels.peer }} not saved for 10+ minutes"

      # Stats history save failure
      - alert: StatsHistorySaveFailed
        expr: rate(tunnelmesh_coordinator_stats_history_save_errors_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          category: data
        annotations:
          summary: "Stats history save failing on coordinator"
          description: "Cannot persist stats history to S3"

      # Stats replication severely lagged (critical)
      - alert: StatsReplicationLag
        expr: time() - tunnelmesh_network_stats_last_save_timestamp > 3600
        for: 5m
        labels:
          severity: critical
          category: data
        annotations:
          summary: "Stats replication severely lagged"
          description: "Stats for {{ $labels.peer }} stale by 1+ hour"

      # Stats corruption detected (critical)
      - alert: StatsCorruptionDetected
        expr: rate(tunnelmesh_coordinator_stats_corruption_detected_total[1m]) > 0
        for: 1m
        labels:
          severity: critical
          category: data
        annotations:
          summary: "Stats file corruption detected"
          description: "Corrupted {{ $labels.stats_type }} stats detected"

      # Docker stats collection completely down (critical)
      - alert: DockerStatsCollectionDown
        expr: tunnelmesh_docker_stats_collection_enabled == 0
        for: 5m
        labels:
          severity: critical
          category: data
        annotations:
          summary: "Docker stats collection disabled"
          description: "Stats collection not running on {{ $labels.peer }}"
